+++
title = "Research"
date = "2024-01-01"
author = "John Abascal"
+++

# Research

My research focuses on **privacy in deep learning** and the intersection of machine learning security and privacy. I work on understanding and mitigating privacy risks in modern machine learning systems.

## Publications

### 2025

**Black-Box Privacy Attacks on Shared Representations in Multitask Learning**  
*John Abascal, Nicolás Berrios, Alina Oprea, Jonathan Ullman, Adam Smith, Matthew Jagielski*  
arXiv preprint arXiv:2506.16460, 2025  
[[arXiv]](https://arxiv.org/abs/2506.16460)

### 2024

**Phantom: General Trigger Attacks on Retrieval Augmented Language Generation**  
*Harsh Chaudhari, Giorgio Severi, John Abascal, Matthew Jagielski, Christopher A. Choquette-Choo, Milad Nasr, Cristina Nita-Rotaru, Alina Oprea*  
arXiv preprint arXiv:2405.20485, 2024  
[[arXiv]](https://arxiv.org/abs/2405.20485)

**TMI! Finetuned Models Leak Private Information from their Pretraining Data**  
*John Abascal, Stanley Wu, Alina Oprea, Jonathan Ullman*  
Proceedings on Privacy Enhancing Technologies (PETS) 2024  
[[arXiv]](https://arxiv.org/abs/2306.01181) [[PDF]](https://arxiv.org/pdf/2306.01181.pdf)

### 2023

**SNAP: Efficient Extraction of Private Properties with Poisoning**  
*H. Chaudhari, J. Abascal, A. Oprea, M. Jagielski, F. Tramèr, J. Ullman*  
IEEE Symposium on Security and Privacy (S&P) 2023  
[[IEEE]](https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.00111) [[Code]](https://www.github.com/johnmath/snap-sp23) [[Video]](https://www.youtube.com/watch?v=p89ZQEt7qhM&t=1s)

> **Abstract**: Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. We design an efficient property inference attack, SNAP, which achieves higher attack success and requires lower amounts of poisoning than prior work. For example, on the Census dataset, SNAP achieves 34% higher success rate while being 56.5x faster.

## Research Interests

- **Privacy-Preserving Machine Learning**: Developing techniques to protect sensitive information in ML models
- **Adversarial Machine Learning**: Understanding vulnerabilities in ML systems and developing defenses
- **Differential Privacy**: Theoretical and practical aspects of privacy-preserving data analysis
- **Property Inference Attacks**: Extracting sensitive properties from machine learning models
- **Data Poisoning**: Attacks that manipulate training data to compromise model behavior

## Current Work

I am currently working as a Student Researcher at **Google** on open problems in machine unlearning, focusing on developing methods to remove specific information from trained models while preserving utility.

## Past Projects

During my internships at **LinkedIn**, I worked on empirically measuring privacy leakage in analytics systems and large language models, developing novel techniques to quantify and mitigate privacy risks in production systems.

---

*For the most up-to-date list of publications, please see my [Google Scholar profile](https://scholar.google.com/citations?user=3jYQ3FsAAAAJ&hl).*