+++
title = "Research"
date = "2025-10-01"
author = "John Abascal"
+++

<!-- # Research

My research focuses on **privacy in deep learning** and the intersection of machine learning security and privacy. I work on understanding and mitigating privacy risks in modern machine learning systems. -->

# Publications

### 2025

**Should We Forget About Certified Unlearning? Evaluating the Pitfalls of Noisy Methods**
  *{{< authors >}}John Abascal, Eleni Triantafillou, Matthew Jagielski, Nicole Mitchell, Peter Kairouz{{< /authors >}}*

**Black-Box Privacy Attacks on Shared Representations in Multitask Learning**  
*{{< authors >}}John Abascal, Nicolás Berrios, Alina Oprea, Jonathan Ullman, Adam Smith, Matthew Jagielski{{< /authors >}}*  
arXiv preprint arXiv:2506.16460, 2025  
{{< pub-buttons arxiv="https://arxiv.org/abs/2506.16460" >}}

### 2024

**Phantom: General Trigger Attacks on Retrieval Augmented Language Generation**  
*{{< authors >}}Harsh Chaudhari, Giorgio Severi, John Abascal, Matthew Jagielski, Christopher A. Choquette-Choo, Milad Nasr, Cristina Nita-Rotaru, Alina Oprea{{< /authors >}}*  
arXiv preprint arXiv:2405.20485, 2024  
{{< pub-buttons arxiv="https://arxiv.org/abs/2405.20485" >}}

**TMI! Finetuned Models Leak Private Information from their Pretraining Data**  
*{{< authors >}}John Abascal, Stanley Wu, Alina Oprea, Jonathan Ullman{{< /authors >}}*  
Proceedings on Privacy Enhancing Technologies (PETS) 2024  
{{< pub-buttons arxiv="https://arxiv.org/abs/2306.01181" pdf="https://petsymposium.org/popets/2024/popets-2024-0075.pdf" code="https://github.com/johnmath/tmi-pets24">}}

### 2023

**SNAP: Efficient Extraction of Private Properties with Poisoning**  
*{{< authors >}}Harsh Chaudhari, John Abascal, Alina Oprea, Matthew Jagielski, Florian Tramèr, Jonathan Ullman{{< /authors >}}*  
IEEE Symposium on Security and Privacy (S&P) 2023  
{{< pub-buttons arxiv="https://arxiv.org/abs/2208.12348" pdf="https://doi.ieeecomputersociety.org/10.1109/SP46215.2023.00111" code="https://www.github.com/johnmath/snap-sp23" video="https://www.youtube.com/watch?v=p89ZQEt7qhM&t=1s" >}}

<details>
<summary><strong>Abstract</strong></summary>

Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. We design an efficient property inference attack, SNAP, which achieves higher attack success and requires lower amounts of poisoning than prior work. For example, on the Census dataset, SNAP achieves 34% higher success rate while being 56.5x faster.

</details>


*For the most up-to-date list of publications, please see my [Google Scholar profile](https://scholar.google.com/citations?user=3jYQ3FsAAAAJ&hl).*
